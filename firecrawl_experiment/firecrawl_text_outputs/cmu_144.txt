Skip to main content
Measuring AI’s Energy and Environmental Footprint
By: Ramayya Krishnan , Mitul Jhaveri and Jay Palat
The rapid expansion of artificial intelligence is driving a surge in data center energy consumption, water use, carbon emissions, and electronic waste — and decision-makers need trusted information about these impacts and how they will change in the future.
Why it matters: Without standardized metrics and reporting, policymakers and grid operators cannot accurately track, manage, or mitigate AI’s growing resource footprint. Given the dramatic differences in energy and resource constraints region-by-region, without open and accurate information, AI's growing energy footprint could lead to significant and unexpected local negative impacts.
Catch up quick: Generative AI and large-scale cloud computing are driving an unprecedented increase in energy demand. But companies often use outdated or narrow measures and purchase renewable energy credits to address sustainability concerns.

Their true carbon footprint may be much higher than the figures they report.
A single hyperscale AI data center can consume hundreds of thousands of gallons of water per day ​ and contribute to a “mountain” of e-waste​, yet only about a quarter of data center operators even track what happens to retired hardware ​.

The big question: If the rapid build-out of AI data centers, on top of other growing power demands, pushes global demand up by an additional hundreds of terawatt hours annually, the steady-growth assumption embedded in today’s models will be shattered.

The International Energy Agency (IEA) forecasts that data center energy use could more than double to 945 TWh by 2030.
Planners need far more granular, forward-looking forecasting methods to avoid driving up costs for rate-payers, last-minute scrambles to find power, and potential electricity reliability crises.

What we're doing: Carnegie Mellon University researchers developed a set of policy recommendations to establish standardized metrics for AI energy and environmental impacts across model training, inference, and data center infrastructure.

Develop AI energy metrics: Congress should direct the Department of Energy (DOE) and the National Institute of Standards and Technology (NIST) to spearhead the creation of a phased plan to develop, implement, and operationalize standardized metrics, in close partnership with industry.
Measure the AI energy lifecycle: NIST should lead a process to create new standardized metrics that capture AI’s energy and environmental footprint across its entire lifecycle — training, inference, data center operations (cooling/power), and hardware manufacturing/disposal.
Mandate reporting: DOE and the Environmental Protection Agency should lead a six‑month voluntary AI energy reporting program, and gradually move toward a mandatory reporting mechanism. The data would feed straight into Energy Information Administration outlooks and Federal Energy Regulatory Commission grid planning.

The bottom line: AI’s extraordinary capabilities should not come at the expense of our energy security or environmental sustainability. By standardizing how we measure AI’s footprint, firms have the incentives to innovate for sustainability and the U.S. can be better prepared for the growth in power consumption while maintaining its leadership in artificial intelligence.
Go deeper: Read the full policy memo CMU wrote for the Federation of American Scientists and the MIT Tech Review article.

jasujazumdinski